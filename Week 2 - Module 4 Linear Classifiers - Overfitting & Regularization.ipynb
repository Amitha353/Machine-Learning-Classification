{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4 - Linear Classifiers : Overfitting & Regularization\n",
    "\n",
    "#### Training the classifier:\n",
    "* The dataset -> (labelled -> [(sentence1, +), (sentence2, -), ...]) is provided.\n",
    "* The dataset is divided into **Training set** and **Validation set**.\n",
    "* The training set is fed into the learn classifier and the algorithm is built, model is trained.\n",
    "* The learnt model is run on the validation set and evaluate the classifier.\n",
    "\n",
    "###  Classification Error\n",
    "* 'Classification error' - It is the measure of the classifiers performance.\n",
    "* If the 'true label' and 'predicted label' are the same then it is right else it is wrong.\n",
    "\n",
    "###### Classification Error\n",
    "* Error measures the fraction of mistakes.\n",
    "* **error = #mistakes / total number of datapoints**.\n",
    "* Best possible value = 0.0.\n",
    "\n",
    "###### Accuracy\n",
    "* Fraction of correct predictions.\n",
    "* **accuracy = #correct / total number of datapoints**.\n",
    "* Best possible value - 1.0.\n",
    "\n",
    "#### Overfitting in regression\n",
    "* Here as the model complexity increases, the model fits the training data very well, but might not be generalized leading to **overfitting**.\n",
    "<IMG SRC=\"overfitting-regression.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting in Classification\n",
    "\n",
    "<img src=\"overfitting-classification.png\">\n",
    "\n",
    "<img src=\"classifation-error-model-complexity.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Overfitting in Classifiers -> Overconfident Predictions\n",
    "\n",
    "* In the 'Logistic Regression Model' the score -> w(transpose) * features -> extendes [-infinity, infinity];\n",
    "* This is reduce to their probability funtion of the output given input using the sigmoin/link function in the range [0,1] -> where the output ranges y [-1, +1];\n",
    "\n",
    "#### The subtle consequences of overfitting in logistic regression\n",
    "* Overfitting leads to 'Large coefficient values'.\n",
    "        - wT * h(xi) -> either very positive / very negative-> which makes the sigmoid(wT * h(xi)) -> go to 0 or 1.\n",
    "* The model becomes extremely overconfident of predictions.   \n",
    "* Below 3 cases are listed for the same \n",
    "    \n",
    "    \n",
    "    * Input : #awesome = 2, #awful = 1\n",
    "    * X-axis = #awesome - #awful = 2 - 1 = 1\n",
    "    \n",
    "    \n",
    "    - Coefficients are intercept, weight -awesome, weight -awful\n",
    "    * Y-axis =  Score = 1 / (1 + e^-(wT * h(xi));\n",
    "    1. Coefficients -> 0, +1, -1  -> 0.73\n",
    "    2. Coefficients -> 0, +2, -2  -> 0.88\n",
    "    3. Coefficients -> 0, +6, -6  -> 0.997\n",
    "    \n",
    "  <img src=\"coefficient-effect-logistic-regression.png\">\n",
    "  \n",
    "#### Visualizing the dataset to identify Overfitting -   Overconfident predictions\n",
    "* As model complexity increases, the decision boundary between the classification data becomes very this indicates overfitting.\n",
    "* The separation between the data must be narrow but not very wide or extremely tiny. **In case of tiny it is an indication of overfitting**.\n",
    "\n",
    "<img src=\"overconfident-dataset.png\">\n",
    "\n",
    "### Another perspective on overfitting logistic regression (ADVANCED)\n",
    "### Linearly-separable data: A dataset that can be classified into categories. A line can be drawn to segregate the data. \n",
    "* Data is linearly separable if:\n",
    "    There are coefficients w-hat such that:\n",
    "        - For all positive training data: Score(x) = wT h(x) >0\n",
    "        - For all negative training data: Score(x) = wT h(x) <0\n",
    "* **training_error(w-hat) = 0. For linearly-separable data the training_error = 0.**\n",
    "*This could be a situation of overfitting - especially w.r.t complex models.*\n",
    "\n",
    "* **Note 1 : If there are D features, linear separability happens in a D-dimensional space.**\n",
    "* **Note 2 : If you have enough features, data are (almost) always linearly separable.** *.\n",
    "* Polynomial to the degree 50,100, 180, etc - data is gonna become linearly separated - lead to Problematic case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Effects of linearly separability on coefficients\n",
    " * Consider a plane that separates the data (positive and negative);\n",
    "     - Plane -> 1.0 #awesome - 1.5 #awful = 0\n",
    "     - Multiplying * 10 -> 10 #awesome - 15 #awful = 0\n",
    "     - Multiplying * 10^9 -> 10x10^9 #awesome - 15x10^9 #awful = 0\n",
    "* In the above case -> although the values of the coefficients is increasing, the plane separating the positive and negative boundaries is still the same. Hence the prediction are not right if its a result of increase in magnitude of the coefficients.\n",
    "\n",
    " **Issue : MLE (Maximum likelihood estimation) - prefers most certain models, but here in case of overfit models / linearly-separable data the coefficients go to infinity, increasing the certaininty of the prediction -> This is problem.**\n",
    " \n",
    "* The picture depicts the effects of high magnitude coefficients on probability.\n",
    "- the point under consideration is near the boundary -> its probability is uncertain and closer to 0.5 and not 1, 0.\n",
    "- But as the magnitude of the coefficients of the model increases its certaininy raises. This leads to false results.\n",
    "\n",
    "<img src=\"linear-separability-overfit.png\">\n",
    "\n",
    "#### Overfitting in logistic regression\n",
    " * Learning tries to find decision boundary that seprates data - 'Overly complex boundary'.\n",
    " * If data is linearly separable -> coefficients go to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### L2 regularized logistic regression\n",
    "\n",
    "#### Penalizing large coefficients to mitigate overfitting\n",
    "* Quality Metric is modified to handle - large coefficinets and prevent overfitting.\n",
    "\n",
    "#### Desired total cost format\n",
    "* Want to balance:\n",
    "    1. How well the function fits the data -> large.\n",
    "    2. Magnitude of the coefficients -> small.\n",
    "    \n",
    "* Total quality = measure of fit - measure of magnitude of coefficients.\n",
    "    - Measure of fit -> data likelihood -> large # = good fit for training data.\n",
    "    - Measure of magnitude of coefficients -> large # = overfit.\n",
    "    \n",
    "#### Part 1 : Maximum likelihood estimate (MLE): \n",
    "* Measure of fit = Data likelihood\n",
    "    * Choose coefficients of w that maximize likelihood.\n",
    "        <img src=\"likelihood.png\">\n",
    "    * Typically, use log of likelihood function (simplifies math and has better gradient/convergence properties.)\n",
    "        <img src=\"natural log.png\">\n",
    "**Data likelihood is supposed to be as big as possible.**\n",
    "\n",
    "#### Part 2 : Measure of magnitude of logistic regression coefficients:\n",
    "* Sum of squares (L2 norm) -> penalize highly positive and highly negative numbers in the same way.\n",
    "* Sum of absolute value (L1 norm) -> provides sparse solutions.\n",
    "\n",
    "* Both the metrics penalize large coefficients.\n",
    "\n",
    "### L2 regularized logistic regression\n",
    "* The mechanism of finding **lambda**/ tuning parameter that balances between the model fit and the coefficient magnitude is **L2 regularized logistic regression**. (In regression case -> term - Ridge Regression);\n",
    "#### Picking lambda:\n",
    "* Validation Set (for large dataset).\n",
    "* Cross-validation (for smaller datasets).\n",
    "#### Bias-Variance tradeoff\n",
    "* Lambda controls the moel complexity.\n",
    "   1. Large lambda : high bias, low variance. (w-hat = 0, lambda = infinity);\n",
    "   2. Small lambda : low bias, high variance. (MLE for higher order polynomial, lambda = 0. \n",
    "<img src=\"l2-regularized-penalty.png\">\n",
    "\n",
    "### L2 regularization address overfitting issues.\n",
    "* Choosing appropriate lambda value with higher order complex models\n",
    "    1. Provides a better decision boundary between the data.\n",
    "    2. The overconfidence predictions is reduced as a natural uncertainity region is obtained.\n",
    "    \n",
    "<img src=\"lambda-tuning.png\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Learning L2 regularized logistic regression with gradient ascent\n",
    "* Algorithm to optimize to get w-hat.\n",
    "* Employ gradient ascent algorithm to find the w-hat.\n",
    "\n",
    "<img src=\"gradient-ascent-l2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse logistic regression with L1 regularization\n",
    "* This provides Efficiency and Interpretability.\n",
    "##### Efficiency:\n",
    "* If size(w-hat) = 100B, each prediction is expensive.\n",
    "*  If w-hat is sparse, computation only depends on the # non-zeros.\n",
    "        y-hat(i) = sign (Sum(wj!=0) w-hatj * hj(xi))\n",
    "##### Interpretabiliy:\n",
    "- Can decipher the features truly relevant for the prediction.\n",
    "\n",
    "#### Sparse Logistic Regression\n",
    "* Total quality = measure of fit - measure of magnitude of coefficients\n",
    "* Total quality - l(w) - ||w||1\n",
    "* L1 norm leads to sparse solutions.\n",
    "\n",
    "#### L1 regularized logistic regression \n",
    "* Lambda is a tuning parameter that provides a balance between the model fit and sparsity.\n",
    "\n",
    "#### Coefficient path - L1 penalty\n",
    "* The coefficients that contribute to the model become 0 eventually with increasing lamba. While those hardly contributing to the model become 0 initially.\n",
    "\n",
    "<img src=\"sparse-logistic-regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "\n",
    "<img src=\"quiz-w2-2-1,2.png\">\n",
    "<img src=\"quiz-w2-2-3,4,5.png\">\n",
    "<img src=\"quiz-w2-2-6.png\">\n",
    "<img src=\"quiz-w2-2-7.png\">\n",
    "<img src=\"quiz-w2-2-8.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
